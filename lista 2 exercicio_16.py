# -*- coding: utf-8 -*-
"""exercicio 16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TfrY4yryjDWAEsIu9hCO3w3v5GuYIsri
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the necessary libraries.
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import random
from matplotlib import cm
import math
# %matplotlib inline

# Always reset the pseudo random number generator to the same number.
seed = 42
np.random.seed(seed)
random.seed(seed)

# Define the number of examples.
N = 1000

# Features.
x1 = 3.0 * np.random.randn(N, 1)
x2 = np.random.randn(N, 1)

# Noise.
w = np.random.randn(N, 1)

# Generate target function.
y = x1 + x2

# Observable model.
y_noisy = y + w

# Generate values for parameters.
M = 200
a1 = np.linspace(-12.0, 14.0, M)
a2 = np.linspace(-12.0, 14.0, M)

A1, A2 = np.meshgrid(a1, a2)

# Generate points for plotting the cost-function surface.
J = np.zeros((M,M))
for iter1 in range(0, M):
    for iter2 in range(0, M):
        yhat = A1[iter1][iter2]*x1 + A2[iter1][iter2]*x2
        J[iter1][iter2] = (1.0/N)*np.sum(np.square(y_noisy - yhat))

# Plot cost-function surface.
fig = plt.figure(figsize=(9,9))
ax = fig.gca(projection='3d')
surf = ax.plot_surface(A1, A2, J, cmap=cm.coolwarm, linewidth=0, antialiased=False)
# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)
ax.set_xlabel('$a_1$', fontsize=14)
ax.set_ylabel('$a_2$', fontsize=14)
ax.set_zlabel('$J_e$', fontsize=14)
plt.title('Cost-function\'s Surface')
plt.savefig("error_surface_sgd_vs_mom.png", dpi=600)
#Show the plot.
plt.show()

# Concatenate both column vectors, x1 and x2.
X = np.c_[x1, x2]

# Closed-form solution.
a_opt = np.linalg.pinv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y_noisy))

yhat = X.dot(a_opt)
Joptimum = (1.0/N)*np.sum(np.power((y_noisy - yhat), 2))

print('Joptimum:',Joptimum)

class SGD():
    
    a = 0
    Jgd = 0
    a_hist = 0
    update_hist = 0
    
    def __init__(self, updateType='conv', N=1000, K=2, n_epochs=1000, alpha=0.02, a_init=[0,0], seed=42, beta=0.99999, epsilon=1e-8):
        self.updateType = updateType
        self.seed = seed
        self.beta = beta
        self.epsilon = epsilon
        self.alpha = alpha
        self.n_epochs = n_epochs
        self.N = N
        self.inc = 0
        self.K = K
        self.a = np.array(a_init).reshape(self.K, 1)
        # Create empty structures.
        self.v_da = np.zeros((self.K,))
        self.a_hist = np.zeros((K, n_epochs*N+1))
        self.v_hist = np.zeros((K, n_epochs*N))
        self.Jgd = np.zeros(n_epochs*N+1)  
        self.alphas = np.zeros((K,n_epochs*N))
        self.update_hist = np.zeros((K, n_epochs*N))
        # Initialize history.
        self.a_hist[:,0] = self.a.reshape(self.K,)
        
    def __update__(self, inc, a, grad):
        if(self.updateType == 'adaGrad'):
            return self.__adaGradUpdate__(inc, a, grad)
        elif(self.updateType == 'RMSProp'):
            return self.__RMSPropUpdate__(inc, a, grad)        
        else:
            return self.__convUpdate__(inc, a, grad)
        
    # Conventional update.
    def __convUpdate__(self, inc, a, grad):
     
        # Update weights.
        update = self.alpha*grad
        a = a - update
        
        # Keep update history.
        self.update_hist[:, inc] = update.reshape(self.K,)
        
        return a

    # AdaGrad-based update.
    def __adaGradUpdate__(self, inc, a, grad):
                
        for k in range(self.K):

            # grad is from current iteration.
            self.v_da[k] = self.v_da[k] +(grad[k,0]**2)
            self.v_hist[k,inc] = self.v_da[k]

            # Update weights.
            alpha_r = self.alpha/(np.sqrt(self.v_da[k]+self.epsilon))
            update = alpha_r*(grad[k,0])
            a[k,0] = a[k,0] - update
        
            # Keep update history.
            self.update_hist[k, inc] = update 
            self.alphas[k, inc] = alpha_r
        
        return a
    
     # RMSProp-based update.
    def __RMSPropUpdate__(self, inc, a, grad):
        
        for k in range(self.K):

            # grad is from current iteration.
            self.v_da[k] = self.beta*self.v_da[k] + (1.0-self.beta)*(grad[k,0]**2)
            self.v_hist[k,inc] = self.v_da[k]

            # Update weights.
            alpha_r = self.alpha/(np.sqrt(self.v_da[k]+self.epsilon))
            update = alpha_r*grad[k,0]
            a[k,0] = a[k,0] - update
        
            # Keep update history.
            self.update_hist[k, inc] = update 
            self.alphas[k, inc] = alpha_r
        
        return a
    
    def getParams(self):
        return self.a, self.Jgd, self.a_hist, self.update_hist, self.alphas, self.v_hist
    
    def fit(self, X, y_noisy):
        
        # Reset PN generator.
        random.seed(self.seed)
        
        # calculate initial error.
        self.Jgd[0] = (1.0/self.N)*sum(np.power(y_noisy - X.dot(self.a), 2))
        
        # Stocastic gradient-descent loop.
        for epoch in range(self.n_epochs):

            # Shuffle the whole dataset before every epoch.
            shuffled_indexes = random.sample(range(0, self.N), self.N)  

            for i in range(self.N):
                # Randomly draw samples from dataset.
                rand_ind = shuffled_indexes[i]
                xi = X[rand_ind:rand_ind+1]
                yi = y_noisy[rand_ind:rand_ind+1]
                
                # Calculate gradient vector.
                grad = -2.0*xi.T.dot(yi - xi.dot(self.a)) 
                
                # Calculate update parameters.
                self.a = self.__update__(self.inc, self.a, grad)

                # Calculate error.
                self.Jgd[self.inc+1] = (1.0/self.N)*sum(np.power(y_noisy - X.dot(self.a), 2))
                
                # Keep history.
                self.a_hist[:, self.inc+1] = self.a.reshape(self.K,)               
                
                # Increment interation counter.
                self.inc += 1
[8]
#c - GDEpuro

# Number of epochs.
n_epochs = 1
# Learning rate.
alpha = 0.02
# Number of features.
K = 2
# Initial weights.
a_init = [-10.0, -10.0]
# Update type.
updateType = 'conv'

# Instantiate traditional SGD.
sgd = SGD(updateType, N, K, n_epochs, alpha, a_init, seed)

# Train traditional SGD.
sgd.fit(X, y_noisy)

# Retrieve parameters.
a_conv, Jgd, a_hist, update_hist, v_hist, alpha_r = sgd.getParams()

#c.i
# Plot figure.        
fig = plt.figure(figsize=(5,5))
cp = plt.contour(A1, A2, J)
plt.clabel(cp, inline=1, fontsize=10)
plt.xlabel('$a_1$', fontsize=14)
plt.ylabel('$a_2$', fontsize=14)
plt.title('Cost-function\'s Contour (Conv. SGD)')
plt.plot(a_opt[0], a_opt[1], c='r', marker='*')
plt.plot(a_hist[0, :], a_hist[1, :], 'kx--')
plt.xticks(np.arange(-10, 14, step=2.0))
plt.yticks(np.arange(-10, 14, step=2.0))
plt.savefig('contourn_conv.png', dpi=600)
plt.show()

#c.ii
fig = plt.figure(figsize=(5,5))

plt.plot(np.arange(0, n_epochs*N), Jgd[0:n_epochs*N])
plt.xlim((0, n_epochs*N))
plt.yscale('log')
plt.xlabel('Iteration')
plt.ylabel('$J_e$')
plt.title('Error vs. Iter. number (Conv. SGD)')
plt.savefig('error_conv.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(15,5))
ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), update_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([-1.5, 1.5])
ax1.set_xlabel('Iteration')
ax1.set_ylabel('$\\nabla_e$')
ax1.set_title('Gradient vs. Iter. number (Conv. SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), update_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-1.5, 1.5])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('$\\nabla_e$')
ax2.set_title('Gradient vs. Iter. number (Conv. SGD)')
ax2.legend()

plt.savefig('grad_conv.png', dpi=600)
plt.show()

#AdaGrad

# Number of epochs.
n_epochs = 1
# Learning rate.
alpha = 0.02
# Number of features.
K = 2
# Initial weights.
a_init = [-10.0, -10.0]
# AdaGrad parameters.
updateType = 'adaGrad'
epsilon = 1e-8

# Instantiate AdaGrad-based SGD.
sgd = SGD(updateType, N, K, n_epochs, alpha, a_init, seed, epsilon)

# Train AdaGrad-based SGD.
sgd.fit(X, y_noisy)

# Retrieve parameters.
a_adaG, Jgd, a_hist, update_hist, v_hist, alpha_r = sgd.getParams()

# Plot figure.        
fig = plt.figure(figsize=(10,5))
cp = plt.contour(A1, A2, J)
plt.clabel(cp, inline=1, fontsize=10)
plt.xlabel('$a_1$', fontsize=10)
plt.ylabel('$a_2$', fontsize=10)
plt.title('Cost-function\'s Contour (AdaGrad SGD)')
plt.plot(a_opt[0], a_opt[1], c='r', marker='*')
plt.plot(a_hist[0, :], a_hist[1, :], 'kx--')
plt.xticks(np.arange(-10, 14, step=2.0))
plt.yticks(np.arange(-10, 14, step=2.0))
plt.savefig('contourn_adaGrad.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

plt.plot(np.arange(0, n_epochs*N), Jgd[0:n_epochs*N])
plt.xlim((0, n_epochs*N))
plt.yscale('log')
plt.xlabel('Iteration')
plt.ylabel('$J_e$')
plt.title('Error vs. Iter. number (AdaGrad SGD)')
plt.savefig('error_adaGrad.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(15,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), update_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([-5, 5])
ax1.set_xlabel('a1')
ax1.set_ylabel('alpha')
ax1.set_title('Gradient vs. Iter. number (AdaGrad SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), update_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-5, 5])
ax2.set_xlabel('a2')
ax2.set_ylabel('alpha')
ax2.set_title('Gradient vs. Iter. number (AdaGrad SGD)')
ax2.legend()

plt.savefig('grad_adaGrad.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), v_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([0, 0.000025])
ax1.set_xlabel('a1')
ax1.set_ylabel('v')
ax1.set_title('Gradient vs. Iter. number (AdaGrad SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot( np.arange(0, n_epochs*N), v_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([0, 0.0004])
ax2.set_xlabel('a2')
ax2.set_ylabel('v')
ax2.set_title('Gradient vs. Iter. number (AdaGrad SGD)')
ax2.legend()

plt.savefig('grad_adaGrad.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), update_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([-1.5, 1.5])
ax1.set_xlabel('Iteration')
ax1.set_ylabel('$\\nabla_e$')
ax1.set_title('Gradient vs. Iter. number (Adam SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), update_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-1.5, 1.5])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('$\\nabla_e$')
ax2.set_title('Gradient vs. Iter. number (Adam SGD)')
ax2.legend()

plt.savefig('grad_adam.png', dpi=600)
plt.show()

#e - RMSProp

# Number of epochs.
n_epochs = 1
# Learning rate.
alpha = 0.02
# Number of features.
K = 2
# Initial weights.
a_init = [-10.0, -10.0]
# RMSProp parameters.
updateType = 'RMSProp'
beta = 0.99999
epsilon = 1e-8

# Instantiate RMSProp-based SGD.
sgd = SGD(updateType, N, K, n_epochs, alpha, a_init, seed, beta, epsilon)

# Train ARMSProp-based SGD.
sgd.fit(X, y_noisy)

# Retrieve parameters.
a_adam, Jgd, a_hist, update_hist, alpha_r, v_hist = sgd.getParams()

v_hist

# Plot figure.        
fig = plt.figure(figsize=(5,5))
cp = plt.contour(A1, A2, J)
plt.clabel(cp, inline=1, fontsize=10)
plt.xlabel('$a_1$', fontsize=14)
plt.ylabel('$a_2$', fontsize=14)
plt.title('Cost-function\'s Contour (RMSProp SGD)')
plt.plot(a_opt[0], a_opt[1], c='r', marker='*')
plt.plot(a_hist[0, :], a_hist[1, :], 'kx--')
plt.xticks(np.arange(-10, 14, step=2.0))
plt.yticks(np.arange(-10, 14, step=2.0))
plt.savefig('contourn_RMSProp.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(8,5))

plt.plot(np.arange(0, n_epochs*N), Jgd[0:n_epochs*N])
plt.xlim((0, n_epochs*N))
plt.yscale('log')
plt.xlabel('Iteration')
plt.ylabel('$J_e$')
plt.title('Error vs. Iter. number (RMSProp SGD)')
plt.savefig('error_RMSProp.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), alpha_r[0,:], 'b', label='$a_1$')
ax1.set_ylim([0.0044, 0.0047])
ax1.set_xlabel('iteration')
ax1.set_ylabel('alpha')
ax1.set_title('Alpha vs. Iter. number (RMSProp SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), alpha_r[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-0.005, 0.15])
ax2.set_xlabel('a2')
ax2.set_ylabel('iteration')
ax2.set_title('1alpha vs. Iter. number (RMSProp SGD)')
ax2.legend()

plt.savefig('grad_RMSProp.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), v_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([18, 20])
ax1.set_xlabel('Iteration')
ax1.set_ylabel('v1')
ax1.set_title('v vs. Iter. number (RMSProp SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), v_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-0.005, 0.15])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('v2')
ax2.set_title('v vs. Iter. number (RMSProp SGD)')
ax2.legend()

plt.savefig('grad_RMSProp.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(alpha_r[0,:], v_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([18, 20])
ax1.set_xlabel('v1')
ax1.set_ylabel('alpha')
ax1.set_title('Alpha vs. v (RMSProp SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(alpha_r[1,:], v_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-0.005, 0.15])
ax2.set_xlabel('v2')
ax2.set_ylabel('alpha')
ax2.set_title('Alpha vs. v (RMSProp SGD)')
ax2.legend()

plt.savefig('grad_RMSProp.png', dpi=600)
plt.show()

fig = plt.figure(figsize=(10,5))

ax1 = fig.add_subplot(121)
ax1.plot(np.arange(0, n_epochs*N), update_hist[0,:], 'b', label='$a_1$')
ax1.set_ylim([-1.5, 1.5])
ax1.set_xlabel('Iteration')
ax1.set_ylabel('$\\nabla_e$')
ax1.set_title('Gradient vs. Iter. number (RMSProp SGD)')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(0, n_epochs*N), update_hist[1,:], 'r--', label='$a_2$')
ax2.set_ylim([-1.5, 1.5])
ax2.set_xlabel('Iteration')
ax2.set_ylabel('$\\nabla_e$')
ax2.set_title('Gradient vs. Iter. number (RMSProp SGD)')
ax2.legend()

plt.savefig('RMSProp SGD.png', dpi=600)
plt.show()

"""No geral as artenativas  convergem e se estabilizam no valor mas baixo, mas sendo queeles tambem possuem suas limitações e tempo e passo de convergência.
O percurso que eles seguem e distinto sendo que possui diferenças e desencaminho, sendo que no final de tudo todos acabam no valor de mínimo global.


 Com relação às iterações, podemos perceber que o algoritmo do RMSProp é o mais rápido para se aproximar do mínimo global, mas todos chegam
em um valor próximo de iterações.

para o adagrad o valor de alpha vai ser menor consoante a variavel V vai crescendo e o valor que o passo de apredizagem vai alterando.


as alternativas convergem e se estabilizam sendo que cada uma com suas imperfeicoes.

"""





